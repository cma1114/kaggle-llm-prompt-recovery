{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8254789,"sourceType":"datasetVersion","datasetId":4740051},{"sourceId":28808,"sourceType":"modelInstanceVersion","modelInstanceId":8332},{"sourceId":40608,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":34190}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Prep for gemma\n!pip install -q -U bitsandbytes==0.42.0\n!pip install -q -U peft==0.8.2\n!pip install -q -U trl==0.7.10\n!pip install -q -U accelerate==0.27.1\n!pip install -q -U datasets==2.17.0\n!pip install -q -U transformers==4.38.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-28T12:17:36.507017Z","iopub.execute_input":"2024-04-28T12:17:36.507335Z","iopub.status.idle":"2024-04-28T12:19:08.876880Z","shell.execute_reply.started":"2024-04-28T12:17:36.507309Z","shell.execute_reply":"2024-04-28T12:19:08.875803Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngcsfs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ns3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb==0.15.11\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nWANDB_API_KEY = UserSecretsClient().get_secret(\"WANDB_API_KEY\") \nwandb.login(key=WANDB_API_KEY)\n%env WANDB_PROJECT=gem7_ft_ds1_v2\nhf_access_token = UserSecretsClient().get_secret(\"HF_AUTH_TOKEN\") ","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:19:20.416194Z","iopub.execute_input":"2024-04-28T12:19:20.416676Z","iopub.status.idle":"2024-04-28T12:19:43.511573Z","shell.execute_reply.started":"2024-04-28T12:19:20.416642Z","shell.execute_reply":"2024-04-28T12:19:43.510488Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting wandb==0.15.11\n  Downloading wandb-0.15.11-py3-none-any.whl.metadata (9.8 kB)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (6.0.1)\nCollecting pathtools (from wandb==0.15.11)\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.11) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb==0.15.11) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.11) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.15.11) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.15.11) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.15.11) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.15.11) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.11) (5.0.1)\nDownloading wandb-0.15.11-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pathtools\n  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=53f3efd0dce7e7ee25cd7b4c5c88252a1e99437964f0eb45eccc6e83cc1a7237\n  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\nSuccessfully built pathtools\nInstalling collected packages: pathtools, wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.16.6\n    Uninstalling wandb-0.16.6:\n      Successfully uninstalled wandb-0.16.6\nSuccessfully installed pathtools-0.1.2 wandb-0.15.11\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"env: WANDB_PROJECT=gem7_ft_ds1_v2\n","output_type":"stream"}]},{"cell_type":"code","source":"#Load base model and prep for lora fine tuning (I'm needing to load on the dual T4s or it runs out of memory in training)\nimport torch\nimport gc\nimport pandas as pd\nimport time\nimport os\nimport random\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n\nmodel=None\ntokenizer=None\ngc.collect()\ntorch.cuda.empty_cache()\n\nmodelName = \"/kaggle/input/gemma/transformers/7b-it/3\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    llm_int8_enable_fp32_cpu_offload=True,\n    bnb_4bit_use_double_quant=True\n)\n\nlora_config = LoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],#, \"gate_proj\", \"up_proj\", \"down_proj\"], #less prone to overfitting w/o mlp modules\n    task_type=\"CAUSAL_LM\",\n    lora_alpha=32,#should be twice r according to lightning ai (hearsay)\n    lora_dropout=0.1,\n    bias=\"none\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(modelName)\nmodel = AutoModelForCausalLM.from_pretrained(modelName, quantization_config=bnb_config, device_map=\"auto\")\n# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\nmodel = prepare_model_for_kbit_training(model)\n###model = PeftModel.from_pretrained(model, \"/kaggle/working/outputs_cdf1/checkpoint-100\",is_trainable=True)#,offload_folder=\"offload/\")\n######model = model.merge_and_unload()\n\nmodel.train()\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:20:42.615203Z","iopub.execute_input":"2024-04-28T12:20:42.616160Z","iopub.status.idle":"2024-04-28T12:23:04.965415Z","shell.execute_reply.started":"2024-04-28T12:20:42.616120Z","shell.execute_reply":"2024-04-28T12:23:04.964473Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a957f804abb8498e9cfff7f98f6cc5c2"}},"metadata":{}},{"name":"stdout","text":"trainable params: 12,845,056 || all params: 8,550,525,952 || trainable%: 0.15022533201007937\n","output_type":"stream"}]},{"cell_type":"code","source":"#read in and format data to train on\nimport pandas as pd\nfrom datasets import Dataset\ndsname = \"/kaggle/input/rewrites/mixed_dataset\"\ndf = pd.read_csv(dsname+\"_train.csv\")\ndf = df.sample(frac=1).reset_index(drop=True)\ndata = Dataset.from_pandas(df)\n\ndef format_prompt(ds):\n    template = (\"The following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` \"\n            \"LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, \"\n            \"and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider \"\n            \"the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with \"\n            \"the prompt that you predict would have yielded that change. Focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\"\n            f\"\\n\\nOriginal Text:\\n{{original_text}}\\n\\nRewritten Text:\\n{{rewritten_text}}\\n\\nPredicted Prompt:\\n{{rewrite_prompt}}\")\n    template = (\"<bos><start_of_turn>user\\nThe following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` \"\n            \"LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, \"\n            \"and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider \"\n            \"the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with \"\n            \"the prompt that you predict would have yielded that change. Focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\"\n            f\"\\n\\nOriginal Text:\\n{{original_text}}\\n\\nRewritten Text:\\n{{rewritten_text}}\\n\\nPredicted Prompt:\\n<end_of_turn><start_of_turn>model\\n{{rewrite_prompt}}<end_of_turn><eos>\")\n    prompts = [template.format(\n        original_text=ot, \n        rewritten_text=rt, \n        rewrite_prompt=rp\n    ) for ot, rt, rp in zip(ds['original_text'], ds['rewritten_text'], ds['rewrite_prompt'])]\n    return {'text': prompts}\nprompts = data.map(format_prompt, batched=True)\nprompts = prompts.remove_columns(data.column_names)\nprint(prompts[0])\n\ndf = pd.read_csv(dsname+\"_val.csv\")\ndf = df.sample(frac=1).reset_index(drop=True)\ndata = Dataset.from_pandas(df)\nprompts_val = data.map(format_prompt, batched=True)\nprompts_val = prompts_val.remove_columns(data.column_names)\nprint(prompts_val[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:23:16.804173Z","iopub.execute_input":"2024-04-28T12:23:16.804756Z","iopub.status.idle":"2024-04-28T12:23:17.386634Z","shell.execute_reply.started":"2024-04-28T12:23:16.804726Z","shell.execute_reply":"2024-04-28T12:23:17.385659Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a0ca42186f747b188e90549fbeb48e4"}},"metadata":{}},{"name":"stdout","text":"{'text': '<bos><start_of_turn>user\\nThe following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with the prompt that you predict would have yielded that change. Focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\\n\\nOriginal Text:\\nGoogle can help you find almost anything, but it’s no good if you’ve lost your smartphone – until today. The search engine now has the ability to check where your phone is directly from its homepage.\\n\\nJust type in “Find my phone,” and Google will show where your phone is on a map. You can then set it to ring, should it be lost under piles of laundry or something of the sort.\\n\\nThere are some caveats: your phone must have the latest version of Android’s main Google app installed, and your browser must be logged into the same Google account your phone is, but it’s a much simpler way to find your phone than going through the Android Device Manager, which many Android users may not even be aware of.\\n\\n➤ Google+ [Google via VentureBeat]\\n\\nRead next: CamFind visual search app for iPhone and Android now features a social network\\n\\nRewritten Text:\\n**Reconstructed text:**\\n\\nGood evening, ladies and gentlemen. I\\'m here to share some exciting news about the latest innovation in technology. Google has introduced a new feature that will revolutionize the way we locate our missing smartphones.\\n\\nIt\\'s called \"Find my phone,\" and it\\'s a game-changer. With this feature, you can now find your phone directly from Google\\'s homepage. Just type in \"Find my phone,\" and the search engine will display a map showing your phone\\'s location.\\n\\nIt\\'s a simple yet effective way to track down your lost device. You can even set it to ring, which is especially useful if you\\'ve misplaced your phone under piles of laundry or something similar.\\n\\nHowever, there are some caveats: your phone must have the latest version of Android\\'s main Google app installed, and your browser must be logged into the same Google account your phone is. But I\\'m sure you\\'ll agree, this is much simpler than going through the Android Device Manager, which many Android users may not even be aware of.\\n\\nSo, the next time you lose your phone, don\\'t panic. Just head over to Google and type in \"Find my phone.\" It\\'s a game-changer!\\n\\nPredicted Prompt:\\n<end_of_turn><start_of_turn>model\\nImagine this text was a presentation in a small town, and Reconstruct it<end_of_turn><eos>'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b4b0d9fc9e24395a5c346af6bfd5aae"}},"metadata":{}},{"name":"stdout","text":"{'text': \"<bos><start_of_turn>user\\nThe following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with the prompt that you predict would have yielded that change. Focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\\n\\nOriginal Text:\\nSingapore Airlines is one of the best air carriers I have ever flown in/with.\\nAmazing service from check-in to being welcomed on board throughout the flight and disembarking.\\nPremium economy and economy classes are really good as well.\\nFood is always amazing.\\nOnly bug bear is there are no travel washbags in business class while you get them in Suite, Premium Economy and Economy.\\nThank you Singapore Airlines.\\n\\nRewritten Text:\\n**Sermon on the Excellence of Singapore Airlines**\\n\\n**Introduction:**\\n\\nBrothers and Sisters, I am here today to share my experience with you about a remarkable journey I recently undertook with Singapore Airlines. As a seasoned traveler, I have flown with numerous airlines, but I must say that Singapore Airlines stands head and shoulders above the rest.\\n\\n**Exceptional Service:**\\n\\nFrom the moment I checked in at the airport to the moment I disembarked, the service provided by Singapore Airlines was impeccable. The staff were friendly, attentive, and went above and beyond to ensure that my every need was met. The welcoming atmosphere on board was unparalleled, and the crew's unwavering hospitality made me feel like a valued guest.\\n\\n**Premium and Economy Classes:**\\n\\nI have flown in both premium economy and economy classes with Singapore Airlines, and I must say that both are exceptional. The premium economy offering is truly a treat, with spacious seats, complimentary champagne, and a host of other amenities. Economy class is also very comfortable, with ample legroom and a host\\n\\nPredicted Prompt:\\n<end_of_turn><start_of_turn>model\\nChange this text into a religious sermon<end_of_turn><eos>\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"#see if we need to trim any outliers\ndf=prompts.to_pandas()\nprint(type(df))\nprint(df.iloc[0])\ndf['tokens'] = df.apply(lambda x: tokenizer(x['text'])['input_ids'], axis=1)\ndf['token_length'] = df['tokens'].apply(len)\nmax_length = df['token_length'].max()\nprint(\"Maximum token length:\", max_length)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nbin_size = 100 \nbins = np.arange(0, max_length + bin_size, bin_size)\n\nplt.figure(figsize=(10, 6))\ndf['token_length'].plot(kind='hist', bins=bins, align='left', rwidth=0.8)\nplt.title('Distribution of Token Lengths')\nplt.xlabel('Token Length')\nplt.ylabel('Frequency')\nplt.xticks(bins)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length=2048\nprint(len(prompts))\ndef filter_tokens(example):\n    tokens = tokenizer(example['text'])['input_ids']\n    return len(tokens) < max_seq_length\nprompts = prompts.filter(filter_tokens)\nprint(len(prompts))\nprint(prompts[0])\n\nprint(len(prompts_val))\ndef filter_tokens(example):\n    tokens = tokenizer(example['text'])['input_ids']\n    return len(tokens) < max_seq_length\nprompts_val = prompts_val.filter(filter_tokens)\nprint(len(prompts_val))\nprint(prompts_val[0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:23:26.715590Z","iopub.execute_input":"2024-04-28T12:23:26.715948Z","iopub.status.idle":"2024-04-28T12:23:29.913980Z","shell.execute_reply.started":"2024-04-28T12:23:26.715921Z","shell.execute_reply":"2024-04-28T12:23:29.913133Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"800\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e14c12afb3d043c99ad3cd284ec4d004"}},"metadata":{}},{"name":"stdout","text":"800\n{'text': '<bos><start_of_turn>user\\nThe following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with the prompt that you predict would have yielded that change. Focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\\n\\nOriginal Text:\\nGoogle can help you find almost anything, but it’s no good if you’ve lost your smartphone – until today. The search engine now has the ability to check where your phone is directly from its homepage.\\n\\nJust type in “Find my phone,” and Google will show where your phone is on a map. You can then set it to ring, should it be lost under piles of laundry or something of the sort.\\n\\nThere are some caveats: your phone must have the latest version of Android’s main Google app installed, and your browser must be logged into the same Google account your phone is, but it’s a much simpler way to find your phone than going through the Android Device Manager, which many Android users may not even be aware of.\\n\\n➤ Google+ [Google via VentureBeat]\\n\\nRead next: CamFind visual search app for iPhone and Android now features a social network\\n\\nRewritten Text:\\n**Reconstructed text:**\\n\\nGood evening, ladies and gentlemen. I\\'m here to share some exciting news about the latest innovation in technology. Google has introduced a new feature that will revolutionize the way we locate our missing smartphones.\\n\\nIt\\'s called \"Find my phone,\" and it\\'s a game-changer. With this feature, you can now find your phone directly from Google\\'s homepage. Just type in \"Find my phone,\" and the search engine will display a map showing your phone\\'s location.\\n\\nIt\\'s a simple yet effective way to track down your lost device. You can even set it to ring, which is especially useful if you\\'ve misplaced your phone under piles of laundry or something similar.\\n\\nHowever, there are some caveats: your phone must have the latest version of Android\\'s main Google app installed, and your browser must be logged into the same Google account your phone is. But I\\'m sure you\\'ll agree, this is much simpler than going through the Android Device Manager, which many Android users may not even be aware of.\\n\\nSo, the next time you lose your phone, don\\'t panic. Just head over to Google and type in \"Find my phone.\" It\\'s a game-changer!\\n\\nPredicted Prompt:\\n<end_of_turn><start_of_turn>model\\nImagine this text was a presentation in a small town, and Reconstruct it<end_of_turn><eos>'}\n200\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b709092dc4240d08405a7260bf65d45"}},"metadata":{}},{"name":"stdout","text":"200\n<bos><start_of_turn>user\nThe following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with the prompt that you predict would have yielded that change. Focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\n\nOriginal Text:\nSingapore Airlines is one of the best air carriers I have ever flown in/with.\nAmazing service from check-in to being welcomed on board throughout the flight and disembarking.\nPremium economy and economy classes are really good as well.\nFood is always amazing.\nOnly bug bear is there are no travel washbags in business class while you get them in Suite, Premium Economy and Economy.\nThank you Singapore Airlines.\n\nRewritten Text:\n**Sermon on the Excellence of Singapore Airlines**\n\n**Introduction:**\n\nBrothers and Sisters, I am here today to share my experience with you about a remarkable journey I recently undertook with Singapore Airlines. As a seasoned traveler, I have flown with numerous airlines, but I must say that Singapore Airlines stands head and shoulders above the rest.\n\n**Exceptional Service:**\n\nFrom the moment I checked in at the airport to the moment I disembarked, the service provided by Singapore Airlines was impeccable. The staff were friendly, attentive, and went above and beyond to ensure that my every need was met. The welcoming atmosphere on board was unparalleled, and the crew's unwavering hospitality made me feel like a valued guest.\n\n**Premium and Economy Classes:**\n\nI have flown in both premium economy and economy classes with Singapore Airlines, and I must say that both are exceptional. The premium economy offering is truly a treat, with spacious seats, complimentary champagne, and a host of other amenities. Economy class is also very comfortable, with ample legroom and a host\n\nPredicted Prompt:\n<end_of_turn><start_of_turn>model\nChange this text into a religious sermon<end_of_turn><eos>\n","output_type":"stream"}]},{"cell_type":"code","source":"example = prompts_val[0]['text'].split(\"<start_of_turn>model\")[0] + \"<start_of_turn>model\"\n#example = \"<start_of_turn>user\\n\" + example + \"<end_of_turn>\\n<start_of_turn>model \"\nprint(example)\nmodel.eval() \nwith torch.no_grad():\n    inputs = tokenizer(example, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n    outputs = model.generate(**inputs,max_new_tokens=60,use_cache=True)#, penalty_alpha=0.6, num_beams=2)\n    rewrite_prompt = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n    print(rewrite_prompt)    \nmodel.train() ","metadata":{"execution":{"iopub.status.busy":"2024-04-27T17:03:23.667921Z","iopub.execute_input":"2024-04-27T17:03:23.668302Z","iopub.status.idle":"2024-04-27T17:04:23.117708Z","shell.execute_reply.started":"2024-04-27T17:03:23.668267Z","shell.execute_reply":"2024-04-27T17:04:23.116788Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<start_of_turn>user The following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with the prompt that you predict would have yielded that change. Focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\n\nOriginal Text:\nHe grew up so fast. He had his whole life ahead of him. Now it's gone. Too soon. \n It feels like only yesterday that I was teaching him how to ride his first two-wheeler. He was so excited. I'll never forget the look on his face. Blue eyes shining with pride looking up at me with that adorable gap-tooth smile. \n Watching that face grow up brought me such joy over the years. His hair slowly going from blonde to a soft brown. His smile filling in with his adult teeth. But his eyes always stayed the same. \n I looked into those eyes now. They did n't shine anymore. They stared back at me dull and lifeless, a milky film spreading over them. They were n't the eyes I loved. I closed them for him so that maybe I could make myself believe he was just sleeping. \n I felt hot tears running down my cheeks, dripping off my chin onto his bloody shirt. It was n't fair. It should have been me. \n It should have been me.\n\n\nRewritten Text:\nSo, here's to the young man who lived a life that was far too short. May he serve as a reminder to us all that life is a fleeting moment, and that the only thing we have is our eyes, which can be as radiant as the stars or as dull as a cold and lifeless pond.\n\nPredicted Prompt:\n <end_of_turn><start_of_turn>model\n","output_type":"stream"},{"name":"stderr","text":"2024-04-27 17:03:31.146685: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-27 17:03:31.146785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-27 17:03:31.418088: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":" Rewrite the essay as a poem model Rewrite the essay as a song model Rewrite the essay as a philosophical essay model Rewrite the essay as a political essay model Rewrite the essay as a comedy model Rewrite the essay as a drama \n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): GemmaForCausalLM(\n      (model): GemmaModel(\n        (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n        (layers): ModuleList(\n          (0-27): 28 x GemmaDecoderLayer(\n            (self_attn): GemmaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): GemmaRotaryEmbedding()\n            )\n            (mlp): GemmaMLP(\n              (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n              (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n              (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n              (act_fn): GELUActivation()\n            )\n            (input_layernorm): GemmaRMSNorm()\n            (post_attention_layernorm): GemmaRMSNorm()\n          )\n        )\n        (norm): GemmaRMSNorm()\n      )\n      (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainerCallback, TrainerControl, TrainerState\n\nclass InferenceCallback(TrainerCallback):\n    def __init__(self, eval_dataset, step_interval=10):\n        self.eval_dataset = eval_dataset\n        self.step_interval = step_interval\n\n    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n        # Run inference every `step_interval` steps\n        if state.global_step % self.step_interval == 0 and state.global_step > 0:\n            # Pick a single example to run inference (change the index as needed)\n            example = self.eval_dataset[0]['text'].split(\"<start_of_turn>model\")[0] + \"<start_of_turn>model\"\n#            example = \"<start_of_turn>user\\n\" + example + \"<end_of_turn>\\n<start_of_turn>model \"\n            model = kwargs['model']\n            model.eval()  # Set the model to evaluation mode\n            with torch.no_grad():\n                inputs = tokenizer(example, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n                outputs = model.generate(**inputs,max_new_tokens=60,use_cache=True)#, penalty_alpha=0.6, num_beams=2)\n                rewrite_prompt = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n                #print(\"Input:\", example)\n                print(f\"Inference output at step {state.global_step}: {rewrite_prompt}\")\n            model.train()  # Set the model back to train mode","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:23:37.945979Z","iopub.execute_input":"2024-04-28T12:23:37.946324Z","iopub.status.idle":"2024-04-28T12:23:37.976548Z","shell.execute_reply.started":"2024-04-28T12:23:37.946300Z","shell.execute_reply":"2024-04-28T12:23:37.975621Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def print_tokens_with_ids(txt):\n    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n    print(list(zip(tokens, token_ids)))\n\nprompt = prompts[0]['text'].split(\"Predicted Prompt:\")[1]\nprint_tokens_with_ids(prompt)  # [..., ('▁Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901), ...]\n\nresponse_template = \"<start_of_turn>model\\n\"\nprint_tokens_with_ids(response_template)  # [('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]","metadata":{"execution":{"iopub.status.busy":"2024-04-27T03:29:39.953999Z","iopub.execute_input":"2024-04-27T03:29:39.954550Z","iopub.status.idle":"2024-04-27T03:29:39.981093Z","shell.execute_reply.started":"2024-04-27T03:29:39.954516Z","shell.execute_reply":"2024-04-27T03:29:39.980097Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7ec38f4ad570>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7ec101407fd0, raw_cell=\"def print_tokens_with_ids(txt):\n    tokens = token..\" store_history=True silent=False shell_futures=True cell_id=3471e9c2-a890-4239-bbbc-d8ba7ab4f2db>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._resume_backend() takes 1 positional argument but 2 were given","output_type":"error"},{"name":"stdout","text":"[('\\n', 108), ('▁', 235248), ('<end_of_turn>', 107), ('<start_of_turn>', 106), ('model', 2516), ('▁Rewrite', 188378), ('▁as', 685), ('▁a', 476), ('▁political', 6269), ('▁commentary', 45007), ('▁using', 2177), ('▁anachron', 234259), ('istic', 4153), ('▁vocabulary', 38853), ('▁', 235248), ('<end_of_turn>', 107)]\n[('<start_of_turn>', 106), ('model', 2516), ('▁', 235248)]\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7ec38f4ad570>> (for post_run_cell), with arguments args (<ExecutionResult object at 7ec101407820, execution_count=25 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7ec101407fd0, raw_cell=\"def print_tokens_with_ids(txt):\n    tokens = token..\" store_history=True silent=False shell_futures=True cell_id=3471e9c2-a890-4239-bbbc-d8ba7ab4f2db> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"#fine tune\nimport transformers\nfrom trl import SFTTrainer,DataCollatorForCompletionOnlyLM\ngc.collect()\ntorch.cuda.empty_cache()\n\nif tokenizer.pad_token is None: tokenizer.pad_token=tokenizer.eos_token\ntokenizer.padding_side = 'right'\nresponse_template = \"<start_of_turn>model\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n# hyperparameters\nbatch_size = 1 #runs out of memory with 2\nlearning_rate=2e-5#2e-4#3e-4 slower leads to less overfitting\nnum_train_epochs=5\nweight_decay=0.0\n\ntraining_arguments = transformers.TrainingArguments(\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        gradient_accumulation_steps=4,\n        gradient_checkpointing=True,\n        num_train_epochs=num_train_epochs,\n        evaluation_strategy=\"steps\",\n        eval_steps=20,\n        warmup_steps=10,\n        save_steps = 100,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir=\"outputs_mxd_cplt\",\n        report_to='wandb',\n        optim=\"paged_adamw_8bit\"\n###        ,resume_from_checkpoint='/kaggle/working/outputs_cdf1/checkpoint-100'\n)\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = prompts,\n    eval_dataset=prompts_val.select(range(100)),\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    packing = False, dataset_num_proc = 2,#True, \n    args=training_arguments,\n    peft_config=lora_config,\n    data_collator=collator,\n    callbacks=[InferenceCallback(eval_dataset=prompts_val, step_interval=20)]\n)\ntrainer.train()\nmodel.push_to_hub(\"cackerman/rewrites_gemma7_ft_ds1\", token = hf_access_token)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:23:46.115831Z","iopub.execute_input":"2024-04-28T12:23:46.116196Z","iopub.status.idle":"2024-04-28T15:01:34.327597Z","shell.execute_reply.started":"2024-04-28T12:23:46.116164Z","shell.execute_reply":"2024-04-28T15:01:34.326313Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-04-28 12:23:48.564554: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-28 12:23:48.564652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-28 12:23:48.696551: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e0b89c37fa463f9a89f74550096d59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80b3d7e5025f42809037a63b7014bc09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-ackerman\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.11"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240428_122404-rnnnodf4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/christopher-ackerman/gem7_ft_ds1_v2/runs/rnnnodf4' target=\"_blank\">icy-dragon-2</a></strong> to <a href='https://wandb.ai/christopher-ackerman/gem7_ft_ds1_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/christopher-ackerman/gem7_ft_ds1_v2' target=\"_blank\">https://wandb.ai/christopher-ackerman/gem7_ft_ds1_v2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/christopher-ackerman/gem7_ft_ds1_v2/runs/rnnnodf4' target=\"_blank\">https://wandb.ai/christopher-ackerman/gem7_ft_ds1_v2/runs/rnnnodf4</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='302' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 302/1000 2:36:19 < 6:03:43, 0.03 it/s, Epoch 1.50/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.509900</td>\n      <td>5.330446</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.690300</td>\n      <td>3.411420</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.713800</td>\n      <td>2.442576</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.948000</td>\n      <td>1.946662</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.066800</td>\n      <td>1.637986</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.236600</td>\n      <td>1.452334</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.990000</td>\n      <td>1.334650</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.278000</td>\n      <td>1.252082</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.037000</td>\n      <td>1.197913</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.763400</td>\n      <td>1.178034</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.059900</td>\n      <td>1.151891</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.599000</td>\n      <td>1.110118</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.871100</td>\n      <td>1.093783</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.035700</td>\n      <td>1.086232</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.554100</td>\n      <td>1.072781</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Inference output at step 20: \n**Write a sermon about the excellence of Singapore Airlines, emphasizing exceptional service, premium and economy classes, and the overall positive experience.**\nInference output at step 40: \nSermon on the Excellence of Singapore Airlines trembling the differences between original text and rewritten text\nInference output at step 60: \nRewrite this essay as a sermon on the excellence of Singapore Airlines as if it were a sermon.\"*\nInference output at step 80: \nRewrite the essay to be a sermon on the excellence of Singapore Airlines as if it were a sermon delivered to a congregation in a church.\"*\nInference output at step 100: \nRewrite Sermon on the Excellence of Singapore Airlines as a Religious Text\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /kaggle/input/gemma/transformers/7b-it/3 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Inference output at step 120: \nRewrite Sermon on the Excellence of Singapore Airlines as a political speech\nInference output at step 140: \nImagine this text was a sermon in a small town, and Adapt it\nInference output at step 160: \nImagine this text was a sermon in a small town, and Revise it\nInference output at step 180: \nImagine this text was a sermon in a small town, and Revise it\nInference output at step 200: \nImagine this text was a sermon in a small town, and Revise it\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /kaggle/input/gemma/transformers/7b-it/3 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Inference output at step 220: \nImagine this text was a sermon in a small town, and Reframe it\nInference output at step 240: \nImagine this text was a sermon in a small town, and Change it\nInference output at step 260: \nImagine this text was a sermon in a small town, and Revise it\nInference output at step 280: \nImagine this text was a sermon in a small town, and Revise it\nInference output at step 300: \nPresent this text into a sermon\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /kaggle/input/gemma/transformers/7b-it/3 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     18\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[1;32m     19\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     20\u001b[0m         per_device_eval_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m###        ,resume_from_checkpoint='/kaggle/working/outputs_cdf1/checkpoint-100'\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     42\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     43\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[InferenceCallback(eval_dataset\u001b[38;5;241m=\u001b[39mprompts_val, step_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)]\n\u001b[1;32m     53\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcackerman/rewrites_gemma7_ft_ds1\u001b[39m\u001b[38;5;124m\"\u001b[39m, token \u001b[38;5;241m=\u001b[39m hf_access_token)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:323\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 323\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1964\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1964\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1966\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"name":"stdout","text":"Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b2d4a938490>> (for post_run_cell), with arguments args (<ExecutionResult object at 7b2ed84b5000, execution_count=7 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 7b2e01583910, raw_cell=\"#fine tune\nimport transformers\nfrom trl import SFT..\" store_history=True silent=False shell_futures=True cell_id=764ee6f5-7fdc-4605-b992-ae8c6f92affc> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"from peft import PeftModel\nmodel=None\ntokenizer=None\ngc.collect()\ntorch.cuda.empty_cache()\n\nmodelName = \"/kaggle/input/gemma/transformers/7b-it/3\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    llm_int8_enable_fp32_cpu_offload=True,\n    bnb_4bit_use_double_quant=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(modelName)\nmodel = AutoModelForCausalLM.from_pretrained(modelName, quantization_config=bnb_config, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model, \"/kaggle/input/gem7_ft_ds2/transformers/cpltsonly_ds2/1\")#\"/kaggle/working/outputs_cdf1/checkpoint-100\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T16:22:15.185352Z","iopub.execute_input":"2024-04-28T16:22:15.185744Z","iopub.status.idle":"2024-04-28T16:22:50.615806Z","shell.execute_reply.started":"2024-04-28T16:22:15.185716Z","shell.execute_reply":"2024-04-28T16:22:50.614739Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b2d4a938490>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7b2b04834430, raw_cell=\"from peft import PeftModel\nmodel=None\ntokenizer=No..\" store_history=True silent=False shell_futures=True cell_id=423d766e-2aa0-4636-ab0b-e7a9ed7b9433>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._resume_backend() takes 1 positional argument but 2 were given","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ead3012a301a41f3a5bafcb87a697428"}},"metadata":{}},{"name":"stdout","text":"Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b2d4a938490>> (for post_run_cell), with arguments args (<ExecutionResult object at 7b2b04834640, execution_count=15 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7b2b04834430, raw_cell=\"from peft import PeftModel\nmodel=None\ntokenizer=No..\" store_history=True silent=False shell_futures=True cell_id=423d766e-2aa0-4636-ab0b-e7a9ed7b9433> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"#run inference\neval_df = pd.read_csv(\"/kaggle/input/rewrites/ood_dataset.csv\")\nfname=\"gem7_ft_cpltsonlyds2_eval_ood.csv\"\ngc.collect()\ntorch.cuda.empty_cache()\ndecoded_outputs = []\noutput_df = pd.DataFrame(columns=[\"original_text\", \"predicted_prompt\", \"true_prompt\"])\noutput_df.to_csv(fname, index=False)\ntest_template = (\"The following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` \"\n            \"LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, \"\n            \"and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider \"\n            \"the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with \"\n            \"the prompt that you predict would have yielded that change. Remember, focus on the *form* not the *content*, and focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\"\n###            f\"{FewShotPrompt}\"\n            f\"\\n\\nOriginal Text:\\n{{original_text}}\\n\\nRewritten Text:\\n{{rewritten_text}}\\n\\nPredicted Prompt:\\n\")\nmax_seq_length=1024\nst=time.time()\nmodel.eval()\nctr=0\nwith torch.no_grad():\n    for idx in range(len(eval_df)):\n        prompt=test_template.format(original_text=eval_df['original_text'][idx], rewritten_text=eval_df['rewritten_text'][idx])\n        prompt = \"<start_of_turn>user\\n\" + prompt + \"<end_of_turn><start_of_turn>model\\n\"# + tokenizer.eos_token\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n        outputs = model.generate(**inputs,max_new_tokens=60,use_cache=True)#, penalty_alpha=0.6, num_beams=2)\n        rewrite_prompt = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n###        rewrite_prompt=rewrite_prompt.split(\" model\")[0].strip()\n###        arr=rewrite_prompt.split(\"\\n\",maxsplit=1)\n###        i = 1 if len(arr) > 1 else 0\n###        rewrite_prompt=arr[i].strip()\n        decoded_outputs.append(rewrite_prompt)\n        if (ctr + 1) % 20 == 0 or idx == len(eval_df) - 1:  # Also save on the last iteration\n            data_partial = {\n                \"original_text\": eval_df[\"original_text\"][idx - 19: idx + 1] if ctr >= 19 else eval_df[\"original_text\"][:idx + 1],\n                \"predicted_prompt\": decoded_outputs[-20:] if ctr >= 19 else decoded_outputs,\n                \"true_prompt\": eval_df[\"rewrite_prompt\"][idx - 19: idx + 1] if ctr >= 19 else eval_df[\"rewrite_prompt\"][:idx + 1]\n            }\n            output_df_partial = pd.DataFrame(data_partial)\n            output_df_partial.to_csv(fname, mode='a', header=False, index=False)\n        print(f\"ctr={ctr}\")\n        ctr+=1\n        if ctr<=3: \n            print(f\"Prompt: {prompt}\")\n            print(f\"Response: {rewrite_prompt}\")\n            \nprint(f\"Elapsed time: {time.time()-st}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-28T16:22:53.787692Z","iopub.execute_input":"2024-04-28T16:22:53.788047Z","iopub.status.idle":"2024-04-28T16:55:23.482516Z","shell.execute_reply.started":"2024-04-28T16:22:53.788014Z","shell.execute_reply":"2024-04-28T16:55:23.481491Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b2d4a938490>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7b2b04c88430, raw_cell=\"#run inference\neval_df = pd.read_csv(\"/kaggle/inpu..\" store_history=True silent=False shell_futures=True cell_id=52f07682-afd5-4d75-8d34-94fe28943174>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._resume_backend() takes 1 positional argument but 2 were given","output_type":"error"},{"name":"stdout","text":"ctr=0\nPrompt: <start_of_turn>user\nThe following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with the prompt that you predict would have yielded that change. Remember, focus on the *form* not the *content*, and focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\n\nOriginal Text:\nnews A senior IT professional specialising in regional telecommunications in Victoria yesterday afternoon delivered an extraordinarily erudite and pointed education to a 3AW radio host who had gone on an extended and inaccurate rant live on air, rebutting claims that the National Broadband Network project would cost $233 billion but deliver speeds no different to ADSL broadband. Yesterday afternoon, the host of 3AW’s Drive program, Tom Elliott, broadcast a segment which contained a large number of factual errors regarding the National Broadband Network project initiated by Labor, which has been continued as the Coalition’s Broadband Network project. Elliott highlighted an\n\nRewritten Text:\n**Procedure:**\n\n**1. Identify the Problem:**\n- Listen to the 3AW radio host's rant and identify the specific factual errors he made regarding the National Broadband Network (NBN) project cost and speed.\n\n**2. Gather Information:**\n- Research the official sources and data on the NBN project, including the government's website, industry reports, and technical documentation.\n- Consult with experts in the field, such as IT professionals and telecommunications engineers.\n\n**3. Prepare a Rebuttal:**\n- Craft a well-structured and erudite response that addresses each of the host's inaccuracies.\n- Use clear and concise language, citing sources and data to support your claims.\n- Break down the technical aspects of the NBN project in a way that is easy for the host to understand.\n\n**4. Delivery:**\n- Contact the host of the 3AW Drive program and offer to provide a guest appearance to deliver your rebuttal.\n- Request an opportunity to address the inaccuracies directly on air.\n- Be prepared to answer questions and engage in a respectful and professional manner.\n\n**5. Follow Up:**\n- After your appearance on the show, follow up with the host to ensure that your points have been understood and incorporated into the broadcast.\n- Monitor the subsequent airwaves to see if the host has corrected his errors and acknowledged your contribution.\n\nPredicted Prompt:\n<end_of_turn><start_of_turn>model\n\nResponse: Imagine this text was a procedure in a big city, and Reframe it\nctr=1\nPrompt: <start_of_turn>user\nThe following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with the prompt that you predict would have yielded that change. Remember, focus on the *form* not the *content*, and focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\n\nOriginal Text:\nTool Announces\"Vicarious\" As New Single, Release Date Set Band Photo: Tool (?) Jonathan Cohen of Billboard.com reports: Tool has revealed the track list for its next studio album, \"10,000 Days.\" As previously reported, the set is due May 2 via Volcano. The first single/album opener \"Vicarious,\" which stretches past the seven-minute mark, is due to arrive April 17 at U.S. rock radio outlets. True to form, the 11-track \"10,000 Days\" sports a number of epic tunes, particularly \"Rosetta Stoned\" and \"10,000 Days (Wings Pt. 2),\" both of which clock in at 11:14. Near the end of the disc are the\n\nRewritten Text:\n**Action:**\n\nListen to the new single \"Vicarious\" by Tool, which is set to be released on April 17th at U.S. rock radio outlets.\n\nPredicted Prompt:\n<end_of_turn><start_of_turn>model\n\nResponse: Imagine this text was a action statement in a haunted house, and Change it\nctr=2\nPrompt: <start_of_turn>user\nThe following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with the prompt that you predict would have yielded that change. Remember, focus on the *form* not the *content*, and focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\n\nOriginal Text:\nRAMALLAH (Reuters) - The body of the Palestinian leader Yasser Arafat will be exhumed on Tuesday, eight years after his death, in an investigation to establish if he was murdered, a Palestinian official said on Saturday. Plastic sheets cover the mausoleum of late Palestinian leader Yasser Arafat in the West Bank city of Ramallah November 24, 2012. REUTERS/Mohamad Torokman A French court opened a murder inquiry in August into Arafat’s death in Paris after a Swiss institute said it had discovered high levels of radioactive polonium on his clothing, which was supplied by his widow, Suha. Tawfiq al-Tirawi, in charge\n\nRewritten Text:\nThis text does not describe any commendable act, therefore I cannot imagine it as a commendation to be commended.\n\nPredicted Prompt:\n<end_of_turn><start_of_turn>model\n\nResponse: Imagine this text was a commendation in a medieval town, and Reframe it\nctr=3\nctr=4\nctr=5\nctr=6\nctr=7\nctr=8\nctr=9\nctr=10\nctr=11\nctr=12\nctr=13\nctr=14\nctr=15\nctr=16\nctr=17\nctr=18\nctr=19\nctr=20\nctr=21\nctr=22\nctr=23\nctr=24\nctr=25\nctr=26\nctr=27\nctr=28\nctr=29\nctr=30\nctr=31\nctr=32\nctr=33\nctr=34\nctr=35\nctr=36\nctr=37\nctr=38\nctr=39\nctr=40\nctr=41\nctr=42\nctr=43\nctr=44\nctr=45\nctr=46\nctr=47\nctr=48\nctr=49\nctr=50\nctr=51\nctr=52\nctr=53\nctr=54\nctr=55\nctr=56\nctr=57\nctr=58\nctr=59\nctr=60\nctr=61\nctr=62\nctr=63\nctr=64\nctr=65\nctr=66\nctr=67\nctr=68\nctr=69\nctr=70\nctr=71\nctr=72\nctr=73\nctr=74\nctr=75\nctr=76\nctr=77\nctr=78\nctr=79\nctr=80\nctr=81\nctr=82\nctr=83\nctr=84\nctr=85\nctr=86\nctr=87\nctr=88\nctr=89\nctr=90\nctr=91\nctr=92\nctr=93\nctr=94\nctr=95\nctr=96\nctr=97\nctr=98\nctr=99\nctr=100\nctr=101\nctr=102\nctr=103\nctr=104\nctr=105\nctr=106\nctr=107\nctr=108\nctr=109\nctr=110\nctr=111\nctr=112\nctr=113\nctr=114\nctr=115\nctr=116\nctr=117\nctr=118\nctr=119\nctr=120\nctr=121\nctr=122\nctr=123\nctr=124\nctr=125\nctr=126\nctr=127\nctr=128\nctr=129\nctr=130\nctr=131\nctr=132\nctr=133\nctr=134\nctr=135\nctr=136\nctr=137\nctr=138\nctr=139\nctr=140\nctr=141\nctr=142\nctr=143\nctr=144\nctr=145\nctr=146\nctr=147\nctr=148\nctr=149\nctr=150\nctr=151\nctr=152\nctr=153\nctr=154\nctr=155\nctr=156\nctr=157\nctr=158\nctr=159\nctr=160\nctr=161\nctr=162\nctr=163\nctr=164\nctr=165\nctr=166\nctr=167\nctr=168\nctr=169\nctr=170\nctr=171\nctr=172\nctr=173\nctr=174\nctr=175\nctr=176\nctr=177\nctr=178\nctr=179\nctr=180\nctr=181\nctr=182\nctr=183\nctr=184\nctr=185\nctr=186\nctr=187\nctr=188\nctr=189\nctr=190\nctr=191\nctr=192\nctr=193\nctr=194\nctr=195\nctr=196\nctr=197\nctr=198\nctr=199\nElapsed time: 1949.2914054393768\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b2d4a938490>> (for post_run_cell), with arguments args (<ExecutionResult object at 7b2b04c88460, execution_count=16 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7b2b04c88430, raw_cell=\"#run inference\neval_df = pd.read_csv(\"/kaggle/inpu..\" store_history=True silent=False shell_futures=True cell_id=52f07682-afd5-4d75-8d34-94fe28943174> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"#Evaluate w/ ST5\n#https://www.kaggle.com/code/richolson/mistral-7b-t5-scoring#Load-sentence-t5-base\n!pip install -Uq sentence_transformers\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import T5EncoderModel\nimport tensorflow as tf\nimport numpy as np\n\nt5_model = SentenceTransformer('sentence-t5-base')\n\n#https://github.com/brohrer/sharpened-cosine-similarity/blob/main/README.md\ndef scs(s: np.ndarray, k: np.ndarray, p: int = 3, q: float = 1e-6):\n    dp = np.dot(s, k)\n    cosine_sim = abs(dp / ((np.linalg.norm(s) + q) * np.linalg.norm(k)))\n    score = np.sign(dp) * (cosine_sim ** p)\n    return score\ndef get_embedding(text):\n    embedding = t5_model.encode(text, convert_to_tensor=True, show_progress_bar=False).cpu().numpy()\n    return embedding.tolist()\n\ndef calculate_t5_distance(embedding1, embedding2):\n    return scs(np.array(embedding1), np.array(embedding2))\n\nt1=\"Rewrite the text to highlight the professionalism and preparedness of the team\"\nt2=\"Rewrite the text as a sci-fi action sequence\"\ntarget=\"Modify the following so as to highlight the professionalism and preparedness of the team\"\nprint(calculate_t5_distance(get_embedding(t1),get_embedding(target)))\nprint(calculate_t5_distance(get_embedding(t2),get_embedding(target)))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T16:18:14.341636Z","iopub.execute_input":"2024-04-28T16:18:14.342019Z","iopub.status.idle":"2024-04-28T16:18:28.972719Z","shell.execute_reply.started":"2024-04-28T16:18:14.341992Z","shell.execute_reply":"2024-04-28T16:18:28.971593Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b2d4a938490>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7b2b046a09a0, raw_cell=\"#Evaluate w/ ST5\n#https://www.kaggle.com/code/rich..\" store_history=True silent=False shell_futures=True cell_id=f8202cf6-00ff-4c35-b016-c95b8aad5572>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._resume_backend() takes 1 positional argument but 2 were given","output_type":"error"},{"name":"stdout","text":"0.8969338572261915\n0.43985375435419966\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b2d4a938490>> (for post_run_cell), with arguments args (<ExecutionResult object at 7b2b046a07f0, execution_count=13 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7b2b046a09a0, raw_cell=\"#Evaluate w/ ST5\n#https://www.kaggle.com/code/rich..\" store_history=True silent=False shell_futures=True cell_id=f8202cf6-00ff-4c35-b016-c95b8aad5572> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nfname=\"/kaggle/working/gem7_ft_cpltsonlyds2_eval_ood.csv\"\ndf=pd.read_csv(fname)\ndf['scs']=df.apply(lambda row: calculate_t5_distance(get_embedding(str(row['predicted_prompt'])), get_embedding(str(row['true_prompt']))), axis=1)\nprint(f\"Avg of {len(df)} prompts is {df['scs'].mean()}\")\n#gem7_ft_cds1: .675 -> 200 steps = .681\n#gem7_base_cds1: .59\n#gem7_base_fewshot_cds1: .6697\n#gem7_ft_cds2: .512; using completions only (v2): .56\n#gem7_base_cds2: .58 \n#gem7_base_fewshot_cds2: .596\n#gem7_ft_custds_eval_on_cd1: .5988\n#gem7_ft_custds_eval_on_cd2: .6857 (200 steps); 0.6886 (100 steps) 0.7087 (300 steps) \n#gem7_ft_mixedds_on_cd2: Avg of 200 prompts is 0.794\n#gem7_ft_mixedds_on_ood: Avg of 200 prompts is 0.706\n#gem7_ft_cpltsonlyds2_on_ood: Avg of 200 prompts is 0.6104150672287963","metadata":{"execution":{"iopub.status.busy":"2024-04-28T16:56:03.622249Z","iopub.execute_input":"2024-04-28T16:56:03.623077Z","iopub.status.idle":"2024-04-28T16:56:09.556169Z","shell.execute_reply.started":"2024-04-28T16:56:03.623041Z","shell.execute_reply":"2024-04-28T16:56:09.555332Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b2d4a938490>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7b2ed84b6d10, raw_cell=\"import pandas as pd\nfname=\"/kaggle/working/gem7_ft..\" store_history=True silent=False shell_futures=True cell_id=6ae8df17-8982-4bcd-bd6d-14902d838393>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._resume_backend() takes 1 positional argument but 2 were given","output_type":"error"},{"name":"stdout","text":"Avg of 200 prompts is 0.6104150672287963\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b2d4a938490>> (for post_run_cell), with arguments args (<ExecutionResult object at 7b2b04839240, execution_count=17 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7b2ed84b6d10, raw_cell=\"import pandas as pd\nfname=\"/kaggle/working/gem7_ft..\" store_history=True silent=False shell_futures=True cell_id=6ae8df17-8982-4bcd-bd6d-14902d838393> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]}]}