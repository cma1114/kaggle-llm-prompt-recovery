# kaggle-llm-prompt-recovery

## Approaches to reverse engineering a prompt
This work was inspired by a Kaggle competition. I joined too late to do much before the contest deadline, but since it was an interesting problem, I continued on with it as a research project and a learning exercise.

The goal is to figure out what rewrite prompt Gemma-7b-it was given, from the original text input to the model and the rewritten text the model output. To make things more challenging, only a single example original-prompt-rewrite triplet was given, so it was necessary to acquire training/eval data. The basic process I followed is given below:

### 1) Data Preparation
- Assemble different training/val sets (generate-training-sets-for-llm-prompt-recovery.ipynb) using:
  - Crowdsourced (from Kaggle users) prompts/texts/rewrites 
  - Self/GPT4-made prompts, crowdsourced texts, and rewrites generated by running those through Gemma-7b-it
  - Notebooks:
    - `generate-training-sets-for-llm-prompt-recovery.ipynb`
    - `Gemma7_Generate_Rewrites.ipynb`

### 2) Training
- Train with different models (Gemma-7b-it, Llama2-7b-chat, Mistral7b-Instruct; quantized due to memory constraints), using various hyperparameters
- Colab/Kaggle notebook training:
  - `fine-tune-with-gemma-7.ipynb`
- Remote server multi-gpu training:
   - `lora_train_gem7_ds.py`
   - `vastai_ds_gem7_setup.sh`

### 3) Evaluation
- Models were evaluated using the same sentence T5/sharpened cosine similarity score as in the competition.
- Evaluated against out-of-distribution (OOD) data, including baseline performances of non-fine-tuned models in zero-shot and few-shot scenarios.
- Evaluation notebook:
  - `model-evaluation.ipynb`

I explored the impacts of differing (in provenance and quality) datasets, diversity of datasets, and dataset sizes, each independently holding all else constant. I also explored different models and hyperparameters, tracking performance in wandb and through interim outputs.

## Insights
In general I found:
- Fine Tuning>Fewshot>Zero-shot
- Bigger models were better
- Diversity of training data was important
- Itâ€™s easy to overfit on the training data; low learning rate helps

Other approaches attempted, without success: Generating multiple responses (through multiple models or through sampling on one model) and asking a bigger model to decide the best one (errors were too correlated, perhaps).


