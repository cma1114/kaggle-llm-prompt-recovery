{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":127736,"sourceType":"datasetVersion","datasetId":64890},{"sourceId":2730445,"sourceType":"datasetVersion","datasetId":1167113},{"sourceId":7563141,"sourceType":"datasetVersion","datasetId":4403839},{"sourceId":7731789,"sourceType":"datasetVersion","datasetId":4509496},{"sourceId":7733314,"sourceType":"datasetVersion","datasetId":4518936},{"sourceId":7742875,"sourceType":"datasetVersion","datasetId":4524732},{"sourceId":7747717,"sourceType":"datasetVersion","datasetId":4506214},{"sourceId":7796657,"sourceType":"datasetVersion","datasetId":4564591},{"sourceId":7835733,"sourceType":"datasetVersion","datasetId":4592972},{"sourceId":7854509,"sourceType":"datasetVersion","datasetId":4606659},{"sourceId":7855873,"sourceType":"datasetVersion","datasetId":4607651},{"sourceId":7868089,"sourceType":"datasetVersion","datasetId":4603723},{"sourceId":7969345,"sourceType":"datasetVersion","datasetId":4689119},{"sourceId":8022464,"sourceType":"datasetVersion","datasetId":4727488},{"sourceId":8247917,"sourceType":"datasetVersion","datasetId":4734348},{"sourceId":11359,"sourceType":"modelInstanceVersion","modelInstanceId":8749},{"sourceId":11382,"sourceType":"modelInstanceVersion","modelInstanceId":8318},{"sourceId":11394,"sourceType":"modelInstanceVersion","modelInstanceId":8332}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Notebook for assembling different training/val sets of crowdsourced and self-made rewrites from crowdsourced and self/ChatGPT-made prompts. \n\nimport numpy as np \nimport pandas as pd \nimport os\nimport gc\nimport re\nimport time\nimport random\nfrom tqdm.auto import tqdm\n\ntqdm.pandas()\npd.set_option('display.max_rows',30)\npd.set_option('display.max_columns',5)\npd.set_option('display.max_colwidth',None)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:03:18.537533Z","iopub.execute_input":"2024-04-28T18:03:18.538389Z","iopub.status.idle":"2024-04-28T18:03:19.527289Z","shell.execute_reply.started":"2024-04-28T18:03:18.538350Z","shell.execute_reply":"2024-04-28T18:03:19.526311Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Make train/test splits\n\nfrom sklearn.model_selection import train_test_split\ndef clean_and_split_data(df, prefix, maxpromptlen, maxtextlen, maxsize=1000, test_size=0.2, val_frac=0.0):\n    df = df[~df['rewrite_prompt'].str.contains('[^\\x00-\\x7F]+', na=False)]\n    df['rewrite_prompt'] = df['rewrite_prompt'].apply(lambda x: re.sub(r'[\\.\\?\\!]+\\s*$', '', re.sub(r'\\s+([?.!,\\':;])', r'\\1', re.sub(r'\\s+', ' ', x.strip()))))\n    df = df[df['rewrite_prompt'].str.len()<=maxpromptlen]\n    df = df[df['original_text'].str.len()<=maxtextlen]\n\n    df = df.sample(frac=1).reset_index(drop=True)#randomly reorder rows\n    df = df[:min(maxsize,len(df))]\n    if test_size == 0.0: #no test set\n        df.to_csv(prefix+'.csv', index=False)\n        return (len(df),0)\n    if val_frac > 0.0: #make separate val and test sets\n        df_train, df_test1 = train_test_split(df, test_size=test_size, random_state=42)\n        df_val, df_test = train_test_split(df, test_size=test_size, random_state=42)\n        df_test.to_csv(prefix+'_test.csv', index=False)\n    else:\n        df_train, df_val = train_test_split(df, test_size=test_size, random_state=42)\n    df_train.to_csv(prefix+'_train.csv', index=False)\n    df_val.to_csv(prefix+'_val.csv', index=False)\n    return (len(df_train),len(df_val))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:04:11.053415Z","iopub.execute_input":"2024-04-28T18:04:11.053889Z","iopub.status.idle":"2024-04-28T18:04:11.628013Z","shell.execute_reply.started":"2024-04-28T18:04:11.053858Z","shell.execute_reply":"2024-04-28T18:04:11.627067Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Make crowdsourced dataset 1\n\ndf_nbv2 = pd.read_csv(\"/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv\")\nprint(df_nbv2.head(1))\nprint(\"Length of dataset:\",len(df_nbv2))\n#remove rows with non-ascii characters\ndf_nbv2 = df_nbv2[~df_nbv2['rewrite_prompt'].str.contains('[^\\x00-\\x7F]+', na=False)]\ndf_nbv2['rewrite_prompt'] = df_nbv2['rewrite_prompt'].apply(lambda x: re.sub(r'[\\.\\?\\!]+\\s*$', '', re.sub(r'\\s+([?.!,\\':;])', r'\\1', re.sub(r'\\s+', ' ', x.strip()))))\narr=sorted(df_nbv2[\"rewrite_prompt\"].unique())\nprint(\"Number of unique prompts:\",len(arr))\nmaxpromptlen=80\nmaxtextlen=4000\narr = [x for x in arr if len(x)<=maxpromptlen]\nprint(\"Number of unique prompts with len<=maxpromptlen:\",len(arr))\nprint(\"Number of orignal texts with len<=maxtextlen:\",len(df_nbv2[df_nbv2['original_text'].str.len()<=3000]))\nprint(\"Number of orignal texts with len<=maxtextlen and len<=80:\",len(df_nbv2[(df_nbv2['original_text'].str.len()<=maxtextlen) & (df_nbv2['rewrite_prompt'].str.len()<=maxpromptlen)]))\n#for l in arr: print(l)\n#print(df_nbv2.iloc[5:10,:])\n(n_train,n_val) = clean_and_split_data(df_nbv2,\"crowdsourced_dataset_1\",maxpromptlen,maxtextlen,maxsize=1000)\nprint(n_train,n_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T19:51:09.003907Z","iopub.execute_input":"2024-04-25T19:51:09.004419Z","iopub.status.idle":"2024-04-25T19:51:09.389649Z","shell.execute_reply.started":"2024-04-25T19:51:09.004372Z","shell.execute_reply":"2024-04-25T19:51:09.388130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make crowdsourced dataset 2\n\ndf_gem70k = pd.read_csv(\"/kaggle/input/70k-prompt-rewrite-triples/70k_gemma_template_built.csv\")\nprint(df_gem70k.head(1))\n#rename generated_text column to 'rewritten_text', 'prompt' to 'rewrite_prompt', drop 'input' column, and add id column\ndf_gem70k = df_gem70k.rename(columns={'generated_text':'rewritten_text','prompt':'rewrite_prompt'})\ndf_gem70k = df_gem70k.drop(columns=['input'])\ndf_gem70k['id'] = df_gem70k.index\nprint(\"Length of dataset:\",len(df_gem70k))\nprint(\"Number of unique prompts:\",len(df_gem70k['rewrite_prompt'].unique()))\nmaxpromptlen=80\nmaxtextlen=4000\nprint(\"Number of unique prompts with len<=maxpromptlen:\",len(df_gem70k[df_gem70k['rewrite_prompt'].str.len()<=maxpromptlen]['rewrite_prompt'].unique()))\nprint(\"Number of orignal texts with len<=maxtextlen:\",len(df_gem70k[df_gem70k['original_text'].str.len()<=3000]))\nprint(\"Number of orignal texts with len<=maxtextlen and maxpromptlen<=80:\",len(df_gem70k[(df_gem70k['original_text'].str.len()<=maxtextlen) & (df_gem70k['rewrite_prompt'].str.len()<=maxpromptlen)]))\n#for l in arr: print(l)\n#print(df_nbv2.iloc[5:10,:])\n(n_train,n_val) = clean_and_split_data(df_gem70k,\"crowdsourced_dataset_2\",maxpromptlen,maxtextlen,maxsize=1000)\nprint(n_train,n_val)\n(n_train,n_val) = clean_and_split_data(df_gem70k,\"crowdsourced_dataset_2_big\",maxpromptlen,maxtextlen,maxsize=10000,test_size=0.02)\nprint(n_train,n_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate custom dataset: 1) Come up with some prompt prefixes\n\nprefix_verbs = ['Adapt',\n                'Change',\n                'Convert',\n                'Craft',\n                'Express',\n                'Frame',\n                'Present',\n                'Recast',\n                'Recreate',\n                'Reformulate',\n                'Rephrase',\n                'Rewrite',\n                'Style',\n                'Transform',\n                'Write']\nprefix_nouns = ['it', \n                'this', \n                'this passage', \n                'this story' ,\n                'this text', \n                'the passage', \n                'the story' ,\n                'the text', \n                'the following passage', \n                'the following story' ,\n                'the following text', \n                'the following']\nprefix_preps = ['as', 'into', 'to be', 'to be like', 'to be more like']\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:29:56.122283Z","iopub.execute_input":"2024-04-26T00:29:56.122750Z","iopub.status.idle":"2024-04-26T00:29:56.134105Z","shell.execute_reply.started":"2024-04-26T00:29:56.122691Z","shell.execute_reply":"2024-04-26T00:29:56.132887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate custom dataset: 2a) Come up with some prompt rewrite styles (with ChatGPT's help)\n\nsuffix_vars = ['a late-night infomercial script',\n               'a dramatic play',\n               'a series of haikus',\n               'a business proposal',\n               'a sci fi setting',\n               'a case study for a successful project',\n               'a sitcom script scene',\n               'a 1920s jazz song',\n               'a Shakespearean sonnet',\n               'a Ted Talk',\n               \"a cheery children's book\",\n               'a classic rock anthem',\n               'a reality TV show plot',\n               'a recipe',\n               'a heartfelt eulogy',\n               'a romance',\n               'a poem',\n               'a noir detective story',\n               'an ancient prophecy',\n               'an exchange between genie and its master',\n               'an online dating profile',\n               'an interview'\n               \"an old sailor's sea shanty\",\n               'a philosophical debate',\n                'a personal diary entry',\n                'a political speech',\n                'a wartime correspondence',\n                'a gothic novel',\n                'a travel guide description',\n                'a superhero comic book',\n                'a silent film screenplay',\n                'a tech startup pitch',\n                'a meditation guide',\n                'a sports commentary',\n                'a high fantasy tale',\n                'a detective noir monologue',\n                'an epic poem',\n                'a steamy love letter',\n                'a horror movie script',\n                'a mockumentary scene',\n                'a self-help book snippet',\n                'a dystopian short story',\n                'an opera libretto',\n                'a slapstick comedy script',\n                'a cyberpunk narrative',\n                'a classical myth retelling',\n                'a spy thriller',\n                'an academic lecture',\n                'a ceremonial speech',\n                'a radio drama script',\n                'a fast-paced thriller',\n                'a historical biography',\n                'a philosophical essay',\n                'a cold war espionage novel',\n                'a documentary script',\n                'a medieval ballad',\n                'an absurdist play',\n                'a technical manual',\n                'a beat poetry reading',\n                'an architectural critique',\n                'a culinary review',\n                'a young adult dystopia',\n                'a rags-to-riches story',\n                'a motivational speech',\n                'a minimalist short story',\n                'a magical realism narrative',\n                'a courtroom drama',\n                'a pulpy adventure novel',\n                'a satirical article',\n                'a fashion magazine feature',\n                'a tragic love story',\n                'a survivalist’s handbook',\n                'a cryptic crossword puzzle',\n                'a Victorian ghost story',\n                'a celebrity interview',\n                'a surrealist painting description',\n                'an urban legend recount',\n                'a public service announcement',\n                'a speculative science essay',\n                'a slapstick comedy routine',\n                'a post-apocalyptic journal',\n                'a cosmic horror story',\n                'a puppet show script',\n                'a sports play-by-play',\n                'a caper story outline',\n                'a confessional poem',\n                'a folk tale retelling',\n                'a limerick sequence',\n                'a viral marketing campaign',\n                'a board game rulebook',\n                'a military strategy analysis',\n                'a religious sermon',\n                'an auctioneer’s chant',\n                'a wilderness survival guide',\n                'a jazz improvisation description',\n                'a graphic novel panel',\n                'a fantasy epic prologue',\n                'a nature documentary narration',\n                'a video game storyline',\n                'an art exhibition review',\n                'a celebrity roast script',\n                'a cyber security briefing']","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:32:19.773117Z","iopub.execute_input":"2024-04-25T23:32:19.773612Z","iopub.status.idle":"2024-04-25T23:32:19.788911Z","shell.execute_reply.started":"2024-04-25T23:32:19.773573Z","shell.execute_reply":"2024-04-25T23:32:19.786986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate custom dataset: 2b) Filter out too semantically similar prompt rewrite styles\n\n#https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing\n!pip install -Uq transformers sentence_transformers faiss-gpu\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom tqdm.autonotebook import tqdm\n\ndef deduplicate_prompts(prompts: list, model: str, threshold: float):\n    sentence_model = SentenceTransformer(model)\n\n    print(\"Converting text to embeddings...\")\n    embeddings = sentence_model.encode(prompts, show_progress_bar=True)\n    dimension = embeddings.shape[1]\n    index = faiss.IndexFlatIP(dimension)\n    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n    index.add(normalized_embeddings)\n\n    print(\"Filtering out near-duplicates...\")\n    D, I = index.search(normalized_embeddings, k=2)\n    to_keep = []\n\n    for i in tqdm(range(len(embeddings)), desc=\"Filtering\"):\n        # If the second closest vector (D[i, 1]) has cosine similarity above the threshold\n        if D[i, 1] >= threshold:\n            # Check if either the current item or its nearest neighbor is already in the to_keep list\n            nearest_neighbor = I[i, 1]\n            if i not in to_keep and nearest_neighbor not in to_keep:\n                # If not, add the current item to the list\n                to_keep.append(i)\n        else:\n            # If the similarity is below the threshold, always keep the current item\n            to_keep.append(i)\n\n    return to_keep\n\nto_keep = deduplicate_prompts(suffix_vars, \"thenlper/gte-large\", 0.9)\ndf_unique = pd.DataFrame(np.array(suffix_vars)[to_keep], columns=['prompt'])\nprint(len(df_unique), len(suffix_vars))\ndf_unique.to_csv(\"deduped_prompts.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:30:50.236487Z","iopub.execute_input":"2024-04-25T23:30:50.237025Z","iopub.status.idle":"2024-04-25T23:31:36.874903Z","shell.execute_reply.started":"2024-04-25T23:30:50.236984Z","shell.execute_reply":"2024-04-25T23:31:36.872901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate custom dataset: 2c) Compute prompt rewrite style probability (to filter for quality; probably overkill)\n\n#load llama-13B\nimport numpy as np \nimport pandas as pd \nimport torch\nimport os\nimport gc\nimport random\nimport time\nfrom kaggle_secrets import UserSecretsClient\nhf_access_token = UserSecretsClient().get_secret(\"HF_AUTH_TOKEN\") \n!pip install -U bitsandbytes\n!pip install -U transformers\n!pip install -U accelerate\n!pip install optimum\nimport optimum\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nfrom torch import cuda, bfloat16\ngc.collect()\ntorch.cuda.empty_cache()\nmodelName = \"meta-llama/Llama-2-13b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(modelName, token=hf_access_token)\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_compute_dtype=torch.bfloat16,bnb_4bit_use_double_quant=True)#,llm_int8_enable_fp32_cpu_offload=True)\nmodel = AutoModelForCausalLM.from_pretrained(modelName, quantization_config=quantization_config, device_map={\"\": 0}, token=hf_access_token)#on T4\n#https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\nprint(model.generation_config)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ndef compute_sent_prob(sent):\n    inputs = tokenizer.encode(sent, return_tensors='pt')\n    outputs = model(inputs, labels=inputs)\n    loss = outputs.loss\n    probability = torch.exp(-loss).item() # The negative loss is the log-likelihood\n    return probability\n    \n#compute this for each prompt in df_prompts, then write to file\ndf_unique=pd.read_csv(\"deduped_prompts.csv\")\ndf_unique['prompt_prob'] = df_unique.iloc[:, 0].progress_apply(compute_sent_prob)\nprint(df_unique.sort_values(by='prompt_prob', ascending=True)[0:10])\ndf_unique.to_csv(\"deduped_prompt_probs.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:04:46.732408Z","iopub.execute_input":"2024-04-28T18:04:46.733012Z","iopub.status.idle":"2024-04-28T18:10:00.837985Z","shell.execute_reply.started":"2024-04-28T18:04:46.732980Z","shell.execute_reply":"2024-04-28T18:10:00.836779Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nCollecting transformers\n  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.20,>=0.19 (from transformers)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\nSuccessfully installed tokenizers-0.19.1 transformers-4.40.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting optimum\n  Downloading optimum-1.19.1-py3-none-any.whl.metadata (19 kB)\nCollecting coloredlogs (from optimum)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.12)\nRequirement already satisfied: transformers<4.41.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (4.40.1)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.1.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.4)\nRequirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.22.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.2.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum) (3.1.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.4.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.2.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (3.20.3)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\nDownloading optimum-1.19.1-py3-none-any.whl (417 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.0/417.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, optimum\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.19.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d9f4ab497d47199621c4b71388a1b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b06ab3b84b436a89dc2a39171184bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83fe5b72d5674df995e341c946c110e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4294b3d28c644dacad9d922d09af273c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50a94341882f4ad6a2959efc48674941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43b4caa8d9d146529f4de6c89df435a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"648c405d544441f4b52da38b3515c6e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b834cf639e56401e93a938b139d04f88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c202460c6934642bf2683101585840c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d17e8c48854a47ff93996409199a5861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4696b984904547b3bef8e0411049e33f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e35c3d73927e4da69bb127743ce1eafd"}},"metadata":{}},{"name":"stdout","text":"GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_length\": 4096,\n  \"pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Generate custom dataset: 3) Combine prompt parts into custom prompts\n\nrephrase_wording_prompts=[]\nfor suffix_var in np.array(suffix_vars)[to_keep]:\n    outstr = suffix_var\n    prepidxs = random.sample(range(len(prefix_preps)), 2)\n    for p in prepidxs:\n        nounidxs = random.sample(range(len(prefix_nouns)), 3)\n        for n in nounidxs:\n            verbidxs = random.sample(range(len(prefix_verbs)), 4)\n            for v in verbidxs:\n                outstr = prefix_verbs[v] + ' ' + prefix_nouns[n] + ' ' + prefix_preps[p] + ' ' + suffix_var\n                rephrase_wording_prompts.append(outstr)\nprint(len(rephrase_wording_prompts))\ndf_rephrase_wording_prompts=pd.DataFrame(rephrase_wording_prompts, columns=['rewrite_prompt'])\ndf_rephrase_wording_prompts = df_rephrase_wording_prompts.sample(frac=1).reset_index(drop=True)\nprint(df_rephrase_wording_prompts.head())\ndf_rephrase_wording_prompts.to_csv(\"custom_rewrite_prompts.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:34:21.378963Z","iopub.execute_input":"2024-04-26T00:34:21.379467Z","iopub.status.idle":"2024-04-26T00:34:21.416271Z","shell.execute_reply.started":"2024-04-26T00:34:21.379431Z","shell.execute_reply":"2024-04-26T00:34:21.414994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate custom dataset: 4) Assemble various sources for \"original texts\" to be rewritten\n\nimport sqlite3\nconn = sqlite3.connect('/kaggle/input/wikibooks-dataset/wikibooks.sqlite')\ncursor = conn.cursor()\ncursor.execute(\"SELECT * from en\")\nraw_eng_text = cursor.fetchall()\ncursor.execute(f\"PRAGMA table_info(en);\")\ncolumn_names = cursor.fetchall()\ncolumn_names = [column[1] for column in column_names]\ndf_eng_text = pd.DataFrame(raw_eng_text, columns=column_names)\ns_abs=df_eng_text[df_eng_text[\"abstract\"].apply(lambda x: len(x) >= 300)][\"abstract\"]\nprint(len(s_abs))\nlengths = s_abs.apply(len)\nprint(f\"Wikibooks Abstracts Max length: {lengths.max()}, Min length: {lengths.min()}, Mean length: {lengths.mean()}\")\n\ndf_em = pd.read_csv(\"/kaggle/input/emotions/text.csv\")\ns_em=df_em[df_em[\"text\"].apply(lambda x: len(x) >= 280)][\"text\"]\nlengths = s_em.apply(len)\nprint(f\"Emotions Length: {len(s_em)}, Max length: {lengths.max()}, Min length: {lengths.min()}, Mean length: {lengths.mean()}\")\n\ndf_tds = pd.read_csv(\"/kaggle/input/1300-towards-datascience-medium-articles-dataset/medium.csv\")\ns_tds=df_tds[df_tds[\"Text\"].apply(lambda x: len(x) <= 2000)][\"Text\"]\nlengths = df_tds.apply(len)\nprint(f\"TDS Length: {len(df_tds)}, Max length: {lengths.max()}, Min length: {lengths.min()}, Mean length: {lengths.mean()}\")\n\ndf_rev = pd.read_csv(\"/kaggle/input/singapore-airlines-reviews/singapore_airlines_reviews.csv\")\ns_rev=df_rev[df_rev[\"text\"].apply(lambda x: len(x) <= 1000 and len(x) >= 300)][\"text\"]\nlengths = s_rev.apply(len)\nprint(f\"Plane Reviews Length: {len(s_rev)}, Max length: {lengths.max()}, Min length: {lengths.min()}, Mean length: {lengths.mean()}\")\n\ndf_movies=pd.read_csv(\"/kaggle/input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv\")\nprint(df_movies.head(1))\nprint(len(df_movies))\nprint(df_movies['Plot'].str.len().mean())\n#number of rows with less than 1000 characters\nprint(len(df_movies[(df_movies['Plot'].str.len() <= 2000) & (df_movies['Plot'].str.len() > 500)]))\ndf_short = df_movies[(df_movies['Plot'].str.len() <= 2000) & (df_movies['Plot'].str.len() > 500)][['Plot']]\n# Rename the 'Plot' column to 'original_text' in the new DataFrame\ndf_short.rename(columns={'Plot': 'original_text'}, inplace=True)\nprint(df_short.head(1))\ndf_short.to_csv(\"movie_plots.csv\", index=False)\ns_plots = df_short['original_text']\n\ndf_mixed = pd.concat([s_tds, s_plots, s_abs, s_em, s_rev], ignore_index=True)\ndf_mixed = df_mixed.sample(frac=1).reset_index(drop=True)#randomly reorder rows\ndf_mixed.columns = ['original_text']\ndf_mixed.to_csv('mixed.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:48:09.240958Z","iopub.execute_input":"2024-04-25T23:48:09.241398Z","iopub.status.idle":"2024-04-25T23:49:24.019723Z","shell.execute_reply.started":"2024-04-25T23:48:09.241369Z","shell.execute_reply":"2024-04-25T23:49:24.017825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate custom dataset: 5) Send prompts and texts through gemma-7 to get rewrites, then create training and validation sets \n\n###https://colab.research.google.com/drive/1UoQeGoXulgO7daG7E83H_-UWZ3UHucks\nmaxpromptlen=80\nmaxtextlen=4000\ndf_cust = pd.read_csv(\"/kaggle/input/gemma7brewrites/gemma7b_custom_prompts_rewrites.csv\")\n(n_train,n_val) = clean_and_split_data(df_cust,\"custom_dataset\",maxpromptlen,maxtextlen,maxsize=1000)\nprint(n_train,n_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:10:40.629009Z","iopub.execute_input":"2024-04-27T20:10:40.629975Z","iopub.status.idle":"2024-04-27T20:10:40.770953Z","shell.execute_reply.started":"2024-04-27T20:10:40.629941Z","shell.execute_reply":"2024-04-27T20:10:40.769821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create mixed dataset to test diversity impact\n\ndf_cust = pd.read_csv(\"/kaggle/working/custom_dataset_train.csv\")\ndf_cd1 = pd.read_csv(\"/kaggle/working/crowdsourced_dataset_1_train.csv\")\ndf_cd2 = pd.read_csv(\"/kaggle/working/crowdsourced_dataset_2_train.csv\")\ndf_cd = pd.concat([df_cust[:267],df_cd1[:267],df_cd2[:266]],axis=0)\ndf_cd.to_csv(\"mixed_dataset_train.csv\",index=False)\ndf_cust = pd.read_csv(\"/kaggle/working/custom_dataset_val.csv\")\ndf_cd1 = pd.read_csv(\"/kaggle/working/crowdsourced_dataset_1_val.csv\")\ndf_cd2 = pd.read_csv(\"/kaggle/working/crowdsourced_dataset_2_val.csv\")\ndf_cd = pd.concat([df_cust[:67],df_cd1[:67],df_cd2[:66]],axis=0)\ndf_cd.to_csv(\"mixed_dataset_val.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:10:35.565792Z","iopub.execute_input":"2024-04-28T12:10:35.566269Z","iopub.status.idle":"2024-04-28T12:10:35.900999Z","shell.execute_reply.started":"2024-04-28T12:10:35.566234Z","shell.execute_reply":"2024-04-28T12:10:35.899772Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Generate ood dataset for testing\n\nmaxpromptlen=80\nmaxtextlen=1000\ndf_ood=pd.read_csv(\"/kaggle/input/data-from-starter/gemma1000_w7b.csv/gemma1000_w7b.csv\")\nprint(df_ood.head(1))\ndf_ood = df_ood.drop(columns=['rewritten_text', 'prompt', 'gemma_7b_rewritten_text_temp0_prefix_removed'])\ndf_ood = df_ood.rename(columns={'gemma_7b_rewritten_text_temp0':'rewritten_text'})\n(n_train,n_val) = clean_and_split_data(df_ood,\"ood_dataset\",maxpromptlen,maxtextlen,maxsize=1000,test_size=0)\nprint(n_train,n_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put together big set for training of final model\n\ndf_cust = pd.read_csv(\"/kaggle/input/gemma7brewrites/gemma7b_custom_prompts_rewrites.csv\")\nprint(\"Cust: \",df_cust.columns)\nprint(len(df_cust))\ndf_gem70k = pd.read_csv(\"/kaggle/input/70k-prompt-rewrite-triples/70k_gemma_template_built.csv\")\ndf_gem70k = df_gem70k.rename(columns={'generated_text':'rewritten_text','prompt':'rewrite_prompt'})\ndf_gem70k = df_gem70k.drop(columns=['input'])\nprint(\"Gem70k: \",df_gem70k.columns)\nprint(len(df_gem70k))\ndf_gem1k = pd.read_csv(\"/kaggle/input/data-from-starter/gemma1000_w7b.csv/gemma1000_w7b.csv\")\ndf_gem1k = df_gem1k.drop(columns=['rewritten_text', 'prompt', 'gemma_7b_rewritten_text_temp0_prefix_removed'])\ndf_gem1k = df_gem1k.rename(columns={'gemma_7b_rewritten_text_temp0':'rewritten_text'})\nprint(\"Gem1k: \",df_gem1k.columns)\nprint(len(df_gem1k))\ndf_nbv2 = pd.read_csv(\"/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv\")\nprint(\"NBV2: \",df_nbv2.columns)\nprint(len(df_nbv2))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:24:54.115140Z","iopub.execute_input":"2024-04-05T14:24:54.115701Z","iopub.status.idle":"2024-04-05T15:41:37.153033Z","shell.execute_reply.started":"2024-04-05T14:24:54.115606Z","shell.execute_reply":"2024-04-05T15:41:37.151004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Screen for quality\n\n#1) Get rid of unnatural/implausible prompts\n\ndf_nbv2['prompt_prob'] = df_nbv2['rewrite_prompt'].progress_apply(compute_sent_prob)\ndf_nbv2.sort_values(by=['prompt_prob'], ascending=True).head(50)[['rewrite_prompt', 'prompt_prob']]\n#remove prompts with prob <0.01, or which contain multiple lines\ndf_nbv2 = df_nbv2[df_nbv2['prompt_prob'] > 0.01]\ndf_nbv2 = df_nbv2[df_nbv2['rewrite_prompt'].str.count('\\n') == 0]\ndf_nbv2.to_csv(\"nbv2_prob.csv\", index=False)\n\n\n#2) Get rid of prompts that led to too little change, or too much\n\n!pip install -Uq transformers sentence_transformers \nfrom sentence_transformers import SentenceTransformer\nfrom tqdm.autonotebook import tqdm\n\ndef scs(s: np.ndarray, k: np.ndarray, p: int = 3, q: float = 1e-6):\n    dp = np.dot(s, k)\n    cosine_sim = abs(dp / ((np.linalg.norm(s) + q) * np.linalg.norm(k)))\n    score = np.sign(dp) * (cosine_sim ** p)\n    return score\n\ndef compute_dists(df: pd.DataFrame, model: str):\n    sentence_model = SentenceTransformer(model)\n\n    print(\"Converting text to embeddings...\")\n    embeddings1 = sentence_model.encode(df['original_text'].tolist(), show_progress_bar=True)\n    embeddings2 = sentence_model.encode(df['rewritten_text'].tolist(), show_progress_bar=True)\n\n    dists = []\n    for i in tqdm(range(len(embeddings1)), desc=\"Computing distances\"):\n        dists.append(scs(np.array(embeddings1[i]), np.array(embeddings2[i])))\n\n    return dists\n\ndf_plot_scored=pd.read_csv(\"/kaggle/working/gem7plotrewrites.csv\")\ndf_plot_scored['scs'] = compute_dists(df_plot_scored, \"thenlper/gte-large\")\ndf_plot_scored = df_plot_scored.sort_values(by=['scs'], ascending=False)\ndf_plot_scored.to_csv(\"gem7plotrewrites_scored.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#choose prompts whose (filtered) similarity between original and rewritten isn't too high\ndf_mixed_scored = pd.read_csv(\"/kaggle/working/gem7mixedrewrites_scored.csv\")\ndf_plot_scored = pd.read_csv(\"/kaggle/working/gem7plotrewrites_scored.csv\")\ndf_mixed_scored = df_mixed_scored[~df_mixed_scored['rewritten_text'].str.startswith(\"I am unable to \")]\ndf_plot_scored = df_plot_scored[~df_plot_scored['rewritten_text'].str.startswith(\"I am unable to \")]\n\ndf_mixed_scored=df_mixed_scored[df_mixed_scored['scs']<0.97]\ndf_plot_scored=df_plot_scored[df_plot_scored['scs']<0.97]\ndf_filtered_prompts = pd.concat([df_mixed_scored['rewrite_prompt'], df_plot_scored['rewrite_prompt']], ignore_index=True)\ndf_filtered_prompts.drop_duplicates(keep='first', inplace=True)\n\n#now combine our generated datasets and filter down to where the prompts are in the filtered set\ndf_mixed = pd.read_csv(\"/kaggle/input/gemma7brewrites/gemma7b_mixed_rewrites.csv\")\nprint(len(df_mixed))\ndf_plots = pd.read_csv(\"/kaggle/input/gemma7brewrites/gemma7b_plot_rewrites.csv\")\nprint(len(df_plots))\ndf_combined = pd.concat([df_mixed, df_plots, df_mixed2], ignore_index=True)#.drop(columns=['scs'])\nprint(len(df_combined))\ndf_combined.drop_duplicates(keep='first', inplace=True)\nprint(len(df_combined))\ndf_combined = df_combined[df_combined['rewrite_prompt'].isin(df_filtered_prompts)]\nprint(len(df_combined))\ndf_combined = df_combined[~df_combined['rewritten_text'].str.startswith(\"I am unable to \")]\nprint(len(df_combined))\ndf_combined['rewritten_text'].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:14:30.487704Z","iopub.execute_input":"2024-04-05T19:14:30.488114Z","iopub.status.idle":"2024-04-05T19:14:30.791166Z","shell.execute_reply.started":"2024-04-05T19:14:30.488067Z","shell.execute_reply":"2024-04-05T19:14:30.789822Z"},"trusted":true},"execution_count":null,"outputs":[]}]}