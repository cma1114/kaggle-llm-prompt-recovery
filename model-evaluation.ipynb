{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8248433,"sourceType":"datasetVersion","datasetId":4740051},{"sourceId":28808,"sourceType":"modelInstanceVersion","modelInstanceId":8332},{"sourceId":39953,"sourceType":"modelInstanceVersion","modelInstanceId":33656},{"sourceId":40472,"sourceType":"modelInstanceVersion","modelInstanceId":34077},{"sourceId":40483,"sourceType":"modelInstanceVersion","modelInstanceId":34088},{"sourceId":40539,"sourceType":"modelInstanceVersion","modelInstanceId":34133},{"sourceId":40598,"sourceType":"modelInstanceVersion","modelInstanceId":34182},{"sourceId":40608,"sourceType":"modelInstanceVersion","modelInstanceId":34190},{"sourceId":40640,"sourceType":"modelInstanceVersion","modelInstanceId":34216}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#load ft'd model for evaluation\n!pip install -q -U accelerate bitsandbytes transformers peft\nimport torch\nimport gc\nimport pandas as pd\nimport time\nimport os\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\nfrom peft import PeftModel, PeftConfig\nbase_model=None\nmodel=None\ntokenizer=None\ngc.collect()\ntorch.cuda.empty_cache()\n\nbase_modelName = \"/kaggle/input/gemma/transformers/7b-it/3\"\ntuned_modelName = \"/kaggle/input/gem7_ft_ds2/transformers/cpltsonly_custds/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_modelName)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n    ,llm_int8_enable_fp32_cpu_offload=True # if you're really pushing the memory threshold\n)\nmodel = AutoModelForCausalLM.from_pretrained(base_modelName, device_map=\"auto\", quantization_config=bnb_config)\n\nconfig = PeftConfig.from_pretrained(tuned_modelName)\nmodel = PeftModel.from_pretrained(model, tuned_modelName)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-27T22:12:36.805369Z","iopub.execute_input":"2024-04-27T22:12:36.805660Z","iopub.status.idle":"2024-04-27T22:14:52.068107Z","shell.execute_reply.started":"2024-04-27T22:12:36.805632Z","shell.execute_reply":"2024-04-27T22:14:52.067261Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Gemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca4920b186e4887a8242856ca15cc74"}},"metadata":{}}]},{"cell_type":"code","source":"#run inference\neval_df = pd.read_csv(\"/kaggle/input/rewrites/crowdsourced_dataset_2_val.csv\")\nfname=\"gem7_ft_custds_eval2.csv\"\ngc.collect()\ntorch.cuda.empty_cache()\ndecoded_outputs = []\noutput_df = pd.DataFrame(columns=[\"original_text\", \"predicted_prompt\", \"true_prompt\"])\noutput_df.to_csv(fname, index=False)\ntrain_df = pd.read_csv(\"/kaggle/input/rewrites/crowdsourced_dataset_2_train.csv\")\nFewShotPrompt = (f\"\\nExamples:\"\n           f\"\\n\\nOriginal Text:\\n{train_df['original_text'][300]}\\n\\nRewritten Text:\\n{train_df['rewritten_text'][300]}\\n\\nPredicted Prompt:\\n{train_df['rewrite_prompt'][300]}\"\n           f\"\\n\\nOriginal Text:\\n{train_df['original_text'][301]}\\n\\nRewritten Text:\\n{train_df['rewritten_text'][301]}\\n\\nPredicted Prompt:\\n{train_df['rewrite_prompt'][301]}\"\n           f\"\\n\\nOriginal Text:\\n{train_df['original_text'][302]}\\n\\nRewritten Text:\\n{train_df['rewritten_text'][302]}\\n\\nPredicted Prompt:\\n{train_df['rewrite_prompt'][302]}\"\n          )\ntest_template = (\"The following `Original Text` passage has been rewritten into `Rewritten Text` by the `Gemma 7b-it` \"\n            \"LLM with a certain prompt. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, \"\n            \"and try to infer the specific prompt that was likely given to the LLM to rewrite the text in this way. Consider \"\n            \"the writing style, meter, tone, etc of the rewritten text, and think about how it differs from the original. Then respond ONLY with \"\n            \"the prompt that you predict would have yielded that change. Remember, focus on the *form* not the *content*, and focus on the DIFFERENCE between the original and rewritten versions, not what is similar.\"\n###            f\"{FewShotPrompt}\"\n            f\"\\n\\nOriginal Text:\\n{{original_text}}\\n\\nRewritten Text:\\n{{rewritten_text}}\\n\\nPredicted Prompt:\\n\")\nmax_seq_length=2048\nst=time.time()\nmodel.eval()\nctr=0\nwith torch.no_grad():\n    for idx in range(len(eval_df)):\n        prompt=test_template.format(original_text=eval_df['original_text'][idx], rewritten_text=eval_df['rewritten_text'][idx])\n        prompt = \"<start_of_turn>user\\n\" + prompt + \"<end_of_turn><start_of_turn>model\\n\"# + tokenizer.eos_token\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n        outputs = model.generate(**inputs,max_new_tokens=60,use_cache=True)#, penalty_alpha=0.6, num_beams=2)\n        rewrite_prompt = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n###        rewrite_prompt=rewrite_prompt.split(\" model\")[0].strip()\n###        arr=rewrite_prompt.split(\"\\n\",maxsplit=1)\n###        i = 1 if len(arr) > 1 else 0\n###        rewrite_prompt=arr[i].strip()\n        decoded_outputs.append(rewrite_prompt)\n        if (ctr + 1) % 20 == 0 or idx == len(eval_df) - 1:  # Also save on the last iteration\n            data_partial = {\n                \"original_text\": eval_df[\"original_text\"][idx - 19: idx + 1] if ctr >= 19 else eval_df[\"original_text\"][:idx + 1],\n                \"predicted_prompt\": decoded_outputs[-20:] if ctr >= 19 else decoded_outputs,\n                \"true_prompt\": eval_df[\"rewrite_prompt\"][idx - 19: idx + 1] if ctr >= 19 else eval_df[\"rewrite_prompt\"][:idx + 1]\n            }\n            output_df_partial = pd.DataFrame(data_partial)\n            output_df_partial.to_csv(fname, mode='a', header=False, index=False)\n        print(f\"ctr={ctr}\")\n        ctr+=1\n        if ctr<=3: \n            print(f\"Prompt: {prompt}\")\n            print(f\"Response: {rewrite_prompt}\")\n            \nprint(f\"Elapsed time: {time.time()-st}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate w/ ST5\n#https://www.kaggle.com/code/richolson/mistral-7b-t5-scoring#Load-sentence-t5-base\n!pip install -Uq sentence_transformers\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import T5EncoderModel\nimport tensorflow as tf\nimport numpy as np\n\nt5_model = SentenceTransformer('sentence-t5-base')\n\n#https://github.com/brohrer/sharpened-cosine-similarity/blob/main/README.md\ndef scs(s: np.ndarray, k: np.ndarray, p: int = 3, q: float = 1e-6):\n    dp = np.dot(s, k)\n    cosine_sim = abs(dp / ((np.linalg.norm(s) + q) * np.linalg.norm(k)))\n    score = np.sign(dp) * (cosine_sim ** p)\n    return score\ndef get_embedding(text):\n    embedding = t5_model.encode(text, convert_to_tensor=True, show_progress_bar=False).cpu().numpy()\n    return embedding.tolist()\n\ndef calculate_t5_distance(embedding1, embedding2):\n    return scs(np.array(embedding1), np.array(embedding2))\n\nt1=\"Rewrite the text to highlight the professionalism and preparedness of the team\"\nt2=\"Rewrite the text as a sci-fi action sequence\"\ntarget=\"Modify the following so as to highlight the professionalism and preparedness of the team\"\nprint(calculate_t5_distance(get_embedding(t1),get_embedding(target)))\nprint(calculate_t5_distance(get_embedding(t2),get_embedding(target)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nsuffix=\"val\"\nfname=\"/kaggle/working/gem7_ft_custds_eval2.csv\"\ndf=pd.read_csv(fname)\ndf['scs']=df.apply(lambda row: calculate_t5_distance(get_embedding(str(row['predicted_prompt'])), get_embedding(row['true_prompt'])), axis=1)\nprint(f\"Avg of {len(df)} prompts is {df['scs'].mean()}\")\n#gem7_ft_cds1: .675 -> 200 steps = .681\n#gem7_base_cds1: .59\n#gem7_base_fewshot_cds1: .6697\n#gem7_ft_cds2: .512; using completions only (v2): .56\n#gem7_base_cds2: .58 \n#gem7_base_fewshot_cds2: .596\n#gem7_ft_custds_eval_on_cd1: .5988\n#gem7_ft_custds_eval_on_cd2: .6857","metadata":{"execution":{"iopub.status.busy":"2024-04-27T22:34:57.737728Z","iopub.execute_input":"2024-04-27T22:34:57.738091Z","iopub.status.idle":"2024-04-27T22:35:03.526988Z","shell.execute_reply.started":"2024-04-27T22:34:57.738063Z","shell.execute_reply":"2024-04-27T22:35:03.526059Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Avg of 200 prompts is 0.6857184627024958\n","output_type":"stream"}]}]}